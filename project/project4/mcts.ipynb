{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from alpha_zero.MCTS import MCTS, PureMCTS\n",
    "from alpha_zero.Coach import Coach\n",
    "from alpha_zero.Arena import Arena\n",
    "from alpha_zero.othello.OthelloGame import OthelloGame\n",
    "from alpha_zero.tictactoe.TicTacToeGame import TicTacToeGame\n",
    "from alpha_zero.othello.pytorch.NNet import NNetWrapper as nn\n",
    "from alpha_zero.utils import dotdict\n",
    "\n",
    "from alpha_zero.tictactoe import TicTacToePlayers\n",
    "from alpha_zero.othello.OthelloPlayers import GreedyOthelloPlayer\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "Implement `select`, `simulate`, `backup`, `ucb_select` of class `PureMCTS` in `alpha_zero/MCTS.py`\n",
    "\n",
    "The following code will check correctness of your implementation, but it might be slightly different with you implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "args = dotdict({'numMCTSSims': 20, 'cpuct':1.0})\n",
    "\n",
    "tictactoe_game = TicTacToeGame(n = 3)\n",
    "board = tictactoe_game.getInitBoard()\n",
    "s = tictactoe_game.stringRepresentation(board)\n",
    "\n",
    "target = Path(\"./data\")\n",
    "\n",
    "def check_select(path, target_actions):\n",
    "    for idx, (path, action) in enumerate(path):\n",
    "        print(path, action)\n",
    "        if action != target_actions[idx]:\n",
    "            raise ValueError(\"path is wrong\")\n",
    "    print(\"select checking: pass\")\n",
    "\n",
    "def check_simulate(reward, target_reward):\n",
    "    if reward != target_reward:\n",
    "        raise ValueError(\"reward is wrong\")\n",
    "    print(\"simulate checking: pass\")\n",
    "\n",
    "def check_backup(Ns, Nsa, Qsa, target_Ns, target_Nsa, target_Qsa):\n",
    "    if Ns != target_Ns:\n",
    "        raise ValueError(\"Ns is wrong\")\n",
    "    if Nsa != target_Nsa:\n",
    "        raise ValueError(\"Nsa is wrong\")\n",
    "    if Qsa != target_Qsa:\n",
    "        raise ValueError(\"Qsa is wrong\")\n",
    "    print(\"backup checking: pass\")\n",
    "\n",
    "for i in range(5):\n",
    "    mcts = PureMCTS(tictactoe_game, args)\n",
    "    mcts.load_tree(target / f\"tree_{i}.npy\")\n",
    "\n",
    "    print(\"=\" * 20 + f\"checking {i}-th tree preset \" + \"=\" * 20)\n",
    "\n",
    "    path, leaf = mcts.select(board)\n",
    "    leaf_s = tictactoe_game.stringRepresentation(leaf)\n",
    "\n",
    "    mcts.expand(leaf, leaf_s)\n",
    "\n",
    "    reward = mcts.simulate(leaf, leaf_s)\n",
    "    mcts.backup(path, reward)\n",
    "\n",
    "    target_actions = np.load(target / f\"tree_select_{i}.npy\", allow_pickle=True)\n",
    "    target_reward = np.load(target / f\"tree_simulate_{i}.npy\", allow_pickle=True)\n",
    "    target_Ns, target_Nsa, target_Qsa = np.load(target / f\"tree_backup_{i}.npy\", allow_pickle=True)\n",
    "\n",
    "    check_select(path, target_actions)\n",
    "    check_simulate(reward, target_reward)\n",
    "    check_backup(mcts.Ns, mcts.Nsa, mcts.Qsa, target_Ns, target_Nsa, target_Qsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect your implemented MCTS will beat random strategy with a high rate\n",
    "\n",
    "There are two hyperparameter your can adjust:\n",
    "* `numMCTSSims`: indicating that how many times rollouts the algorithm will do for picking action\n",
    "* `cpuct`: parameter to balance exploration and exploitation in UCB selection. Higher cpuct results in more exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "total_matches = 100\n",
    "\n",
    "args = dotdict({'numMCTSSims': 50, 'cpuct': 1.0})\n",
    "mcts = PureMCTS(tictactoe_game, args)\n",
    "\n",
    "random_player = TicTacToePlayers.RandomPlayer(tictactoe_game).play\n",
    "mcts_player = lambda x: np.argmax(mcts.getActionProb(x, temp=0))\n",
    "arena = Arena(mcts_player, random_player, tictactoe_game, display=TicTacToeGame.display)\n",
    "\n",
    "win, lose, tie = arena.playGames(total_matches, verbose=False)\n",
    "\n",
    "print(f\"vs random win: {win}, tie: {tie}, lose: {lose}\")\n",
    "if (win + tie) > total_matches * 0.95:\n",
    "    print(\"Implamentation of MCTS is totally correct\")\n",
    "else:\n",
    "    raise Exception(\"Implamentation of MCTS might be wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaZero\n",
    "\n",
    "Implement `simulate`, `ucb_select` of class `MCTS` in `alpha_zero/MCTS.py`\n",
    "Implement `executeEpisode` of class `Coach` in `alpha_zero/Coach.py`\n",
    "\n",
    "There are 5 more hyperparameter your can adjust:\n",
    "* `numIters`: number of iteration to train nnet\n",
    "* `numEps`: number of complete self-play games to simulate during a new iteration.\n",
    "* `tempThreshold`: first `tempThreshold` steps in `executeEpisode` will use temp as 1\n",
    "* `updateThreshold`: win rate threshould to accept the new network or not\n",
    "* `maxlenOfQueue`: size of history pool\n",
    "\n",
    "Usually there is no need to adjust these hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "training_args = dotdict({\n",
    "    'numIters': 10,\n",
    "    'numEps': 100,\n",
    "    'tempThreshold': 15,\n",
    "    'updateThreshold': 0.6,\n",
    "    'maxlenOfQueue': 200000,\n",
    "    'numMCTSSims': 25,\n",
    "    'arenaCompare': 40,\n",
    "    'cpuct': 1,\n",
    "\n",
    "    'checkpoint': './othello_6/',\n",
    "    'load_model': False,\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "})\n",
    "\n",
    "othello_game = OthelloGame(n = 6)\n",
    "\n",
    "nnet = nn(othello_game)\n",
    "\n",
    "c = Coach(othello_game, nnet, training_args)\n",
    "\n",
    "c.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect your implemented MCTS will beat greedy strategy with a high rate\n",
    "\n",
    "tie with MCTS algorithm, but with higher speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "total_matches = 40\n",
    "\n",
    "args = dotdict({'numMCTSSims': 50, 'cpuct': 1.0})\n",
    "othello_game = OthelloGame(n = 6)\n",
    "nnet = nn(othello_game)\n",
    "\n",
    "nnet.load_checkpoint(\"othello_6\", \"best.pth.tar\")\n",
    "alpha_zero = MCTS(othello_game, nnet, args)\n",
    "mcts = PureMCTS(othello_game, args)\n",
    "\n",
    "greed_player = GreedyOthelloPlayer(othello_game).play\n",
    "mcts_player = lambda x: np.argmax(mcts.getActionProb(x, temp=0))\n",
    "your_alpha_zero_player = lambda x: np.argmax(alpha_zero.getActionProb(x, temp=0))\n",
    "\n",
    "arena = Arena(your_alpha_zero_player, greed_player, othello_game, display=OthelloGame.display)\n",
    "\n",
    "win, lose, tie = arena.playGames(total_matches, verbose=False)\n",
    "print(f\"vs greed win: {win}, tie: {tie}, lose: {lose}\")\n",
    "if win <= total_matches * 0.8:\n",
    "    raise Exception(\"Implamentation of alphaZero might be wrong or the hyperparameters are not good enough\")\n",
    "\n",
    "arena = Arena(your_alpha_zero_player, mcts_player, othello_game, display=OthelloGame.display)\n",
    "\n",
    "win, lose, tie = arena.playGames(total_matches, verbose=False)\n",
    "print(f\"vs mcts win: {win}, tie: {tie}, lose: {lose}\")\n",
    "if win >= total_matches * 0.4:\n",
    "    print(\"Implamentation of MCTS is totally correct\")\n",
    "else:\n",
    "    raise Exception(\"Implamentation of alphaZero might be wrong or the hyperparameters are not good enough\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
