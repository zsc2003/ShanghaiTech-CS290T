{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edffb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a254702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the environment and plotting functions\n",
    "sys.path.insert(0,'./env')\n",
    "sys.path.insert(0,'./utils')\n",
    "from env.food_collector import Food_Collector_Env\n",
    "from utils.plot_results import plot_scores_epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4da5d4",
   "metadata": {},
   "source": [
    "# Independent Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ddaf3",
   "metadata": {},
   "source": [
    "## Define QL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a89e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "\n",
    "    def __init__(self, num_actions, num_states, eps_start=1.0, eps_decay=.9999, eps_min=1e-08, step_size=0.1, gamma=1):\n",
    "        # Initialise agent\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        self.epsilon = eps_start\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.rand_generator = np.random.RandomState(1)\n",
    "\n",
    "        # Create an array for action-value estimates and initialize it to zero.\n",
    "        self.state_dict = {}\n",
    "        self.q = np.zeros((self.num_states, self.num_actions)) # The array of action-value estimates.\n",
    "\n",
    "\n",
    "    def agent_start(self, state):\n",
    "\n",
    "        #Update epsilon at the start of each episode\n",
    "        self.epsilon = max(self.epsilon * self.eps_decay, self.eps_min)\n",
    "\n",
    "\n",
    "        #Add state to dict if new + get index of state\n",
    "        ############################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        ############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Choose action using epsilon greedy.\n",
    "        ############################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        ############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.prev_state_idx = self.state_dict[state]\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "\n",
    "        #Add state to dict if new + get index of state\n",
    "        ############################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        ############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Choose action using epsilon greedy.\n",
    "        ############################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        ############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Update q values\n",
    "        self.q[self.prev_state_idx][self.prev_action] =  ######## YOUR IMPLEMENTATION HERE ########\n",
    "\n",
    "\n",
    "        self.prev_state_idx = self.state_dict[state]\n",
    "        self.prev_action = action\n",
    "        return action\n",
    "\n",
    "\n",
    "    def agent_end(self, state, reward):\n",
    "        #Add state to dict if new + get index\n",
    "        ############################\n",
    "        # YOUR IMPLEMENTATION HERE #\n",
    "        ############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Perform the last q value update\n",
    "        self.q[self.prev_state_idx][self.prev_action] =  ######## YOUR IMPLEMENTATION HERE ########\n",
    "\n",
    "\n",
    "\n",
    "    def argmax(self, q_values):\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.rand_generator.choice(ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93666fc7",
   "metadata": {},
   "source": [
    "## Train independent agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_QL_agents(n_agents, num_episodes, max_steps_done, eps_decay, eps_min, step_size, gamma):\n",
    "\n",
    "    # initiate environment\n",
    "    env = Food_Collector_Env(grid_size=[11,11], n_agents=2)\n",
    "\n",
    "    # get numer of states and actions from environment\n",
    "    n_states = 3000000\n",
    "    n_actions = env.action_space[0].n\n",
    "\n",
    "    # Initialise agents\n",
    "    agents = []\n",
    "    for _ in range(n_agents):\n",
    "        QL_agent = QLearningAgent(num_actions=n_actions, num_states=n_states,\n",
    "                    eps_start = 1.0, eps_decay=eps_decay, eps_min=eps_min,\n",
    "                    step_size=step_size, gamma=gamma)\n",
    "        agents.append(QL_agent)\n",
    "\n",
    "    # Monitor the scores and epsilon for each episode\n",
    "    episode_rewards = [[] for _ in range(n_agents)]\n",
    "    epsilon_history = list()\n",
    "    won_games = 0\n",
    "\n",
    "    # for episode in num_episodes\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        rewards_temp = [ [] for _ in range(n_agents)]\n",
    "\n",
    "        # get initial state and actions\n",
    "        states = env.reset()\n",
    "        for i in range(len(agents)):\n",
    "            action = agents[i].agent_start(states[i])\n",
    "\n",
    "        rewards = [0 for _ in range(n_agents)]\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            steps += 1\n",
    "            actions = []\n",
    "            for i in range(n_agents):\n",
    "                action = agents[i].agent_step(rewards[i], states[i])\n",
    "                actions.append(action)\n",
    "\n",
    "            next_states, rewards, done, info = env.step(actions)\n",
    "\n",
    "            for i in range(n_agents):\n",
    "                rewards_temp[i].append(rewards[i])\n",
    "\n",
    "            if done[0]:\n",
    "                won_games += 1\n",
    "                for i in range(n_agents):\n",
    "                    agents[i].agent_end(states[i], rewards[i]) #update q values last time\n",
    "\n",
    "                if episode % 1000 == 0:\n",
    "                    for i in range(n_agents):\n",
    "                        episode_rewards[i].append(sum(rewards_temp[i]))\n",
    "                epsilon_history.append(agents[0].epsilon)\n",
    "                break\n",
    "\n",
    "            if steps >= max_steps_done:\n",
    "                if episode % 1000 == 0:\n",
    "                    for i in range(n_agents):\n",
    "                        episode_rewards[i].append(sum(rewards_temp[i]))\n",
    "                epsilon_history.append(agents[0].epsilon)\n",
    "                break\n",
    "\n",
    "            states = next_states\n",
    "    print(f'Games won: {won_games}, in percentage: {(100 * won_games / num_episodes):.2f}')\n",
    "    return agents, episode_rewards, epsilon_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2107b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "N_AGENTS = 2\n",
    "NUM_EPISODES = 600000\n",
    "EPS_DECAY = 0.9999\n",
    "EPS_MIN = 0.3\n",
    "STEP_SIZE = 0.1\n",
    "GAMMA = 0.99\n",
    "MAX_STEPS_DONE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f13c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents, reward_history, epsilon_history = train_QL_agents(N_AGENTS, NUM_EPISODES, MAX_STEPS_DONE, EPS_DECAY,\n",
    "                                                            EPS_MIN, STEP_SIZE, GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b972d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores_epsilon(reward_history, epsilon_history, moving_avg_window = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbef08e",
   "metadata": {},
   "source": [
    "## Save agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5552964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "# Save the file\n",
    "dill.dump(agents[0], file = open(\"trained_agents/QLagent1.pickle\", \"wb\"))\n",
    "dill.dump(agents[1], file = open(\"trained_agents/QLagent2.pickle\", \"wb\"))\n",
    "\n",
    "\n",
    "# Load the file\n",
    "# a1 = dill.load(open(\"trained_agents/QLagent1.pickle\", \"rb\"))\n",
    "# a2 = dill.load(open(\"trained_agents/QLagent2.pickle\", \"rb\"))\n",
    "# agents2 = [a1,a2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_0231",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
